---
title: "Epistasis project interactions analysis"
author: "Armand González"
date: "5/26/2021"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: "journal"
    number_sections: true
    fig_captions: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r clear_global_envir, echo=FALSE}
rm(list = ls())
```

```{r load_library, echo=FALSE}
library(kableExtra)
library(rlist)
library(xlsx)
library(ggplot2)
```

</br>

# Preparation

</br>

## Genotyped data set preparation

</br>

First we define input and output path shortcuts.

```{r defining_paths}
input <- file.path(getwd(), "data", "Input")
output <- file.path(getwd(), "data", "Output")
```

Then we delete the the output folder and all the files and folders it contains
from the previous run.

```{r deleting_output}
unlink(output, recursive = TRUE)
```

The following code chunk consists in the arguments that this R script can take from the user. "verbose" means whether to output to the terminal, "imputated" is if the program should expect an imputated dataset "EP" performs needed data correction before analysis in the [Epistasis Project dataset](https://digital.csic.es/bitstream/10261/216256/1/BullidoM_TheEpistasisProject.pdf) and "ADNI" means if the input dataset also includes the [ADNI dataset](http://adni.loni.usc.edu/data-samples/data-types/).

```{r arguments}
verbose <- FALSE
```

Then, we proceed to read and store the genotyped input data.

```{r read_geno}
encoded_NA <- c("00", "", "???", "-9")
df <- read.csv2(file.path(input, "genotyped.csv"),
                na.strings = encoded_NA)

df$DHFR_rs70991108_INDEL <- NULL # Deleting indel from the dataset
df$PPARG_rs709149 <- NULL # Genes with unclear genotyping were removed because of suspected error
df$MS4A4E_rs670139 <- NULL # Genes with unclear genotyping were removed because of suspected error
df <- df[rowSums(is.na(df)) != ncol(df), ] # Removing rows that are all NA's
str(df, list.len = 30)
```

Reading the snps that we want to study

```{r read_snps}
vector_of_gene_snps <- scan(file.path(input, "snps.txt"),
                            what = "character",
                            quiet = !verbose)
vector_of_gene_snps
length(vector_of_gene_snps)
```

Chosing other variables of interest 
```{r variables_of_interest}
variables <- c("ID", "Diag", "SexLett", "Age", "Centre", "LGC_E4.")
```

Subsetting the dataframe with the desired variables

```{r subsetting_interest}
df <- df[, c(variables, vector_of_gene_snps)]
dim(df)
```

After that, we order the genotypes so that in the heterozygotes are always sorted by alphabetical order ("A", "C", "G", "T").

```{r geno_alphabetical}
df[,c(vector_of_gene_snps)] <- lapply(df[,c(vector_of_gene_snps)], order_heterozygotes)
```

Then we create a list of objects that for each SNP contains information about its name, gene it belongs, genotype counts, genotype frequencies, allele frequencies and counts, minor allele and major allele etc.

```{r generate_object, results='hide'}
list_of_objects <- generate_object(exists = exists('list_of_objects'),
                                   dataset = df,
                                   snps = vector_of_gene_snps,
                                   verbose = verbose)
```

```{r print_SNP_set}
list_of_objects
```

Each of the SNP objects present in the list_of_objects have the following structure:

```{r print_SNP}
list_of_objects[[1]]
```

We can extract just the reference snp id from this object

```{r vector_snps}
vector_of_snps <- sapply(list_of_objects, function(x) x$id)
vector_of_snps <- unname(vector_of_snps)
vector_of_snps
```

And then we can convert the categorical variables in the dataframe into the factor datatype.

```{r convert_factor}
df[-c(1,4)] <- lapply(df[-c(1,4)], as.factor)
```

</br>

## Imputated data set preparation

</br>

First we read and store the the imputated imput data.

```{r read_imput}
encoded_NA <- c("#NULL!", "NA")
imput_df <- read.csv2(file.path(input, "imputated.csv"), 
                      na.strings = encoded_NA)
str(imput_df, list.len = 30)
```

Then we subset the data of interest.

```{r subset_imput_interest}
imput_variables <- c("id", "DiagName", "sex", "Age.to.Use", "E4status")
imput_df <- imput_df[,c(imput_variables, vector_of_snps)]
dim(imput_df)
```

Then, we import data from the allele schema used in the imputation, in order to reconvert it back to genotypes.

```{r reference}
imput_scheme <- read.csv(file.path(input, "imputation_scheme.csv"), sep = ";")
imput_scheme <- imput_scheme[imput_scheme$SNP %in% vector_of_snps,]
colnames(imput_scheme) <- c("snp_id", "reference", "alternative")
```

There is still some snp that is not found in the reference scheme. It seems some of the snps in the dataset were directly genotyped and codified as dummy allele dosage data, so we will use the human reference genome GRCh37 to recodify them.

```{r genotyped_snps}
genotyped_snps <- setdiff(vector_of_snps, imput_scheme$snp_id)
genotyped_snps
```

```{r GRCh37}
GRCh37 <- read.table(file.path(input, "GRCh37_snps.txt"), quote = "\"", comment.char = "")
colnames(GRCh37) <- c("chromosome", "location", "snp_id", "reference", "alternative")
index <- nrow(imput_scheme)
for (snp in genotyped_snps) {
  i <- which(genotyped_snps == snp)
  imput_scheme[index + i,] <- GRCh37[GRCh37$snp_id == snp,][,c(3,4,5)]
}
row.names(imput_scheme) <- NULL
```


So the final dataframe used to recodify the imputated data into genotypes will be the following one:

```{r imput_scheme_2.0, eval=FALSE}
imput_scheme
```

```{r imput_scheme_2.0_print, echo=FALSE}
imput_scheme[order(imput_scheme$snp_id),] %>% kbl(row.names = F) %>% kable_paper(c("hover"))
```

First, though the imputation data is converted to the numeric datatype.

```{r to_numeric}
imput_df[, 6:ncol(imput_df)] <- lapply(imput_df[,6:ncol(imput_df)], 
                                       function(x) as.numeric(x))
```

Then we proceed to the recoding of the imputation data into genotyped one.

```{r genotype_imputated}
imput_df <- genotype_imputated_df(list_of_objects = list_of_objects, 
                                  df = imput_df, 
                                  scheme = imput_scheme,
                                  match_strands = T,
                                  verbose = verbose)
```

Then again like we did with the genotyped dataset, we order the genotypes so that in the heterozygotes are always sorted by alphabetical order ("A", "C", "G", "T").

```{r imput_alphabetical}
imput_df[,c(vector_of_snps)] <- lapply(imput_df[,c(vector_of_snps)], order_heterozygotes)
```

And we convert data types from characters to factors

```{r convert_factor2}
imput_df[-c(1,4)] <- lapply(imput_df[-c(1,4)], as.factor)
```

</br>

## Merging imputated and genotyped datasets

</br>

Renaming variables names in the datasets

```{r rename}
colnames(df) <- c("ID", "Diag", "Sex", "Age_to_use", "Centre", "E4status", vector_of_snps)
colnames(imput_df) <- c("ID", "Diag", "Sex", "Age_to_use", "E4status", vector_of_snps)
```

As we are studying LOAD, subset cases higher or equal than 60 years old in the datasets

```{r higher_than_60}
df_old <- df
imput_df_old <- imput_df
df <- df[df$Age_to_use >= 60,]
imput_df <- imput_df[imput_df$Age_to_use >= 60,]
```

And we can see that `r nrow(df_old) - nrow(df)` cases were removed from the genotype dataset, and `r nrow(imput_df_old) - nrow(imput_df)` from the imputated.

```{r removed_young}
nrow(df_old) - nrow(df)
# 81

nrow(imput_df_old) - nrow(imput_df)
# 0
```

We can calculate the median of the combined population
```{r median_pop}
median(c(df$Age_to_use, imput_df$Age_to_use))
# 82
```

Renaming the levels of E4status in the genotyped dataset.

```{r rename_levels_E4status}
levels(df$E4status)
levels(df$E4status) <- c("E4-", "E4+")
```

And the levels of Diagnosis in the imputated dataset.

```{r rename_levels_Diag}
levels(imput_df$Diag)
levels(imput_df$Diag) <- c("Control", "AD")
```

We also create a binary variable in both datasets that tells whether the age of the individual is equal or above the median of the population. In this case, as we saw earlier it is `r median(c(df$Age_to_use, imput_df$Age_to_use))`.

```{r Age82}
df$Age82 <- ifelse(df$Age_to_use >= 82, ">82", "<82")
imput_df$Age82 <- ifelse(imput_df$Age_to_use >= 82, ">82", "<82")
```

We found a case of unknown gender in the imputated dataset, so we remove it and reset the levels. And then rename them as "Male" and "Female".

```{r unknown_gender}
table(imput_df$Sex)
imput_df <- imput_df[imput_df$Sex != 1,]
imput_df$Sex <- factor(imput_df$Sex) 
levels(imput_df$Sex) <- c("Male", "Female")
```

And we create a new variable "Centre" to keep track that this cases are from the center of Rotterdam.

```{r rotterdam}
imput_df$Centre <- "ROTTERDAM"
```

The last step before merging is extracting the data from Rotterdam to fill up the list_of_objects created in a previous step.

```{r list_of_objects_RT}
list_of_objects <- generate_object(exists = exists('list_of_objects'), 
                                   dataset = imput_df, 
                                   snps = vector_of_snps)
```

Then, the final structure obtained would be similar to the following one:

```{r final_str_list_of_objects}
str(list_of_objects)
```

Once we've done that, we can proceed with the merging.

```{r merging}
matching_variables <- c("ID", "Diag", "Sex", "E4status", "Age_to_use", "Age82", "Centre", vector_of_snps)
All <- merge(x = df, y = imput_df, by = matching_variables, all = T)
str(All, list.len = 30)
```

Before doing any analysis, we should make sure the contrasts are performed with respect to the controls, not the Alzheimer cases.

```{r reference_controls}
All$Diag <- relevel(All$Diag, ref = "Control")
```

Adding binary variables to the merged dataset according to if the data
belong to a specific region, being 0 not belonging and 1 belonging.

```{r centres}
centres <- levels(All$Centre)
centres <- stringr::str_to_title(centres)
All[centres] <- 0
for (centre in centres) {
  All[[centre]][All$Centre == toupper(centre)] <- 1
}
```

Then we divide the dataset into regions according to the centres.

```{r regions}
All$Region <- ifelse(All$Centre == "MADRID" | All$Centre == "OVIEDO" | All$Centre == "SANTANDER", "Spain", "N.Eur")
```


## Quality control

We will perform a quality control of the samples before proceeding to the analysis step. An example of typical quality control procedures usually performed can be found [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6001694/pdf/MPR-27-e1608.pdf)

### Formatting dataset

Once we've merged both datasets we can proceed with quality control procedures.

We've previosly prepared a map file with the snps under study. It was generated with the GRCh38 genome assembly version.

```{r read_map}
map <- read.delim(file.path(input, "raw_data.map"), header = FALSE)
colnames(map) <- c("Chr", "SNP", "GD", "BPP")
```

The file has the following structure:

```{r map_file, eval=FALSE}
map
```

```{r map_file_print, echo=FALSE}
map %>% kbl(row.names = F) %>% kable_paper(c("hover"))
```

We can see that the snps ids are exactly the same found in our dataset so we already got all the information we need for the map file.

```{r equal_snps}
setdiff(map$SNP, names(list_of_objects))
setdiff(names(list_of_objects), map$SNP)
```

Now, we take care of the ped file. First, we create an empty ped file structure that we will fill up with the data.

```{r create_ped}
ped <- data.frame(matrix(ncol = 6, nrow = nrow(All)))
colnames(ped) <- c("FID", "IID", "PID", "MID", "Sex", "P")
```

Then we introduce the columns values for each of the defined variables

```{r fill_ped}
ped$FID <- All$ID #It is the same plink does when the flag --no-fid is used
ped$IID <- All$ID
ped$PID <- 0
ped$MID <- 0
ped$Sex <- sapply(All$Sex, function(x) {if (is.na(x)) {0} else if (x == "Male") {1} else if (x == "Female") {2} else {0}})
ped$P <- sapply(All$Diag, function(x) {if (is.na(x)) {-9} else if (x == "Control") {1} else if (x == "AD") {2} else {-9}})
```

If we take look we can see that the variables have been coded correctly

```{r sanity_check}
head(All[,c("ID", "Sex", "Diag")])
head(ped[,c("IID", "Sex", "P")])
```

Now, then the final step would be to add the genotypes, and as the version of plink we are using (PLINK v1.90) parses the genotypes, we don't need to split the columns we have into allele columns and we can just bind it directly to our ped file.

```{r add_genotypes}
ped <- cbind(ped, All[,map$SNP])
```

But we have yet some missing values remaining in our dataset.

```{r missing_values}
colSums(is.na(ped))
```

And we convert the missing values to 00, as plink expects.

```{r recode_NA}
ped[,map$SNP] <- lapply(ped[,map$SNP], as.character)
ped[is.na(ped)] <- "00"
sum(is.na(ped))
```

Then we can save the ped file in order to use it as input for plink.

```{r write_ped}
write.table(ped, file = file.path(input, "raw_data.ped"), quote = F, sep = "\t", row.names = F, col.names = F)
```


### Checking plink version

<!-- Check whether the platform is unix or windows, as the commands would be slightly different -->

```{r check_platform, echo=FALSE}
is.windows <- ifelse(.Platform$OS.type == "windows", TRUE, FALSE)
is.unix <- ifelse(.Platform$OS.type == "unix", TRUE, FALSE)
```

We will use the command line plink program. To get the version, you can just type:

```{bash plink_version, echo=is.unix, eval=is.unix}
plink --version 
# PLINK v1.90b6.21 64-bit (19 Oct 2020)
```

```{r plink_version2, echo=is.windows, eval=is.windows}
run_shell("plink --version")
# PLINK v1.90b6.22 64-bit (16 Apr 2021)
```

### Making binary files

The first step is to create a folder where to store the output that we will obtain in all subsequent steps using the program. 


```{bash make_dir_plink, echo=is.unix, eval=is.unix}
mkdir -p "$PWD"/data/Output/plink/0-Data
```

```{r make_dir_plink2, echo=is.windows, eval=is.windows}
run_shell('mkdir %CD%/data/Output/plink/0-Data')
```

The next step step will be to convert the files obtained into binary files to speed up the computation.

```{bash binary_files, echo=is.unix, eval=is.unix}
plink --file "$PWD"/data/Input/raw_data --make-bed --out "$PWD"/data/Output/plink/0-Data/binary_data
```

```{r binary_files2, echo=is.windows, eval=is.windows}
run_shell('plink --file %CD%/data/Input/raw_data --make-bed --out %CD%/data/Output/plink/0-Data/binary_data', ignore.stdout = !verbose)
```

### Removing missingness

We first filter SNPs and individuals based on a relaxed threshold (0.2; >20%), as this will filter out SNPs and individuals with very high levels of missingness. Then a filter with a more stringent threshold will be applied (0.02). We do first SNP filtering before individual filtering.

```{bash missing_plink, eval=is.unix, echo=is.unix}
mkdir -p "$PWD"/data/Output/plink/1-QC/1-Missingness
plink --bfile "$PWD"/data/Output/plink/0-Data/binary_data --missing --out "$PWD"/data/Output/plink/1-QC/1-Missingness/missing
plink --bfile "$PWD"/data/Output/plink/0-Data/binary_data --geno 0.2 --make-bed --out "$PWD"/data/Output/plink/1-QC/1-Missingness/missing_1
plink --bfile "$PWD"/data/Output/plink/1-QC/1-Missingness/missing_1 --mind 0.2 --make-bed --out "$PWD"/data/Output/plink/1-QC/1-Missingness/missing_2
plink --bfile "$PWD"/data/Output/plink/1-QC/1-Missingness/missing_2 --geno 0.02 --make-bed --out "$PWD"/data/Output/plink/1-QC/1-Missingness/missing_3
plink --bfile "$PWD"/data/Output/plink/1-QC/1-Missingness/missing_3 --mind 0.02 --make-bed --out "$PWD"/data/Output/plink/1-QC/1-Missingness/missing_4
```

```{r missing_plink2, echo=is.windows, eval=is.windows}
run_shell('mkdir %CD%/data/Output/plink/1-QC/1-Missingness')
run_shell('plink --bfile %CD%/data/Output/plink/0-Data/binary_data --missing --out %CD%/data/Output/plink/1-QC/1-Missingness/missing', ignore.stdout = !verbose)
run_shell('plink --bfile %CD%/data/Output/plink/0-Data/binary_data --geno 0.2 --make-bed --out %CD%/data/Output/plink/1-QC/1-Missingness/missing_1', ignore.stdout = !verbose)
run_shell('plink --bfile %CD%/data/Output/plink/1-QC/1-Missingness/missing_1 --mind 0.2 --make-bed --out %CD%/data/Output/plink/1-QC/1-Missingness/missing_2', ignore.stdout = !verbose)
run_shell('plink --bfile %CD%/data/Output/plink/1-QC/1-Missingness/missing_2 --geno 0.02 --make-bed --out %CD%/data/Output/plink/1-QC/1-Missingness/missing_3', ignore.stdout = !verbose)
run_shell('plink --bfile %CD%/data/Output/plink/1-QC/1-Missingness/missing_3 --mind 0.02 --make-bed --out %CD%/data/Output/plink/1-QC/1-Missingness/missing_4', ignore.stdout = !verbose)
```

### Removing SNPs with lower MAF than threshold

SNPs with a low MAF are rare, therefore power is lacking for detecting SNP‐phenotype associations. These SNPs are also more prone to genotyping errors. The MAF threshold depends on sample size, larger samples can use lower MAF thresholds. As the sample size in this study is moderate (N = 8716), we use the 0.05 as MAF threshold.

```{bash MAF, eval=is.unix, echo=is.unix}
echo "$PWD"
mkdir "$PWD"/data/Output/plink/1-QC/2-MAF
plink --bfile "$PWD"/data/Output/plink/1-QC/1-Missingness/missing_4 --maf 0.05 --make-bed --out "$PWD"/data/Output/plink/1-QC/2-MAF/minor_allele
```

```{r MAF2, echo=is.windows, eval=is.windows}
run_shell('mkdir %CD%/data/Output/plink/1-QC/2-MAF')
run_shell('plink --bfile %CD%/data/Output/plink/1-QC/1-Missingness/missing_4 --maf 0.05 --make-bed --out %CD%/data/Output/plink/1-QC/2-MAF/minor_allele', ignore.stdout = !verbose)
```

### Looking at deviations from Hardy-Weinberg equilibrium

Deviations from HWE is a common indicator of genotyping error, but it may also indicate evolutionary selection. We will use a threshold of 10^-6 for controls and 10^-10 for cases.

```{bash HWE, eval=is.unix, echo=is.unix}
echo "$PWD"
mkdir "$PWD"/data/Output/plink/1-QC/3-HWE
plink --bfile "$PWD"/data/Output/plink/1-QC/2-MAF/minor_allele --hwe 1e-6 --make-bed --out "$PWD"/data/Output/plink/1-QC/3-HWE/hardy_weinberg
plink --bfile "$PWD"/data/Output/plink/1-QC/3-HWE/hardy_weinberg --hwe 1e-10 include-nonctrl --make-bed --out "$PWD"/data/Output/plink/1-QC/3-HWE/hardy_weinberg_1'
```

```{r HWE2, echo=is.windows, eval=is.windows}
run_shell('mkdir %CD%/data/Output/plink/1-QC/3-HWE')
run_shell('plink --bfile %CD%/data/Output/plink/1-QC/2-MAF/minor_allele --hwe 1e-6 --make-bed --out %CD%/data/Output/plink/1-QC/3-HWE/hardy_weinberg', ignore.stdout = !verbose)
run_shell('plink --bfile %CD%/data/Output/plink/1-QC/3-HWE/hardy_weinberg --hwe 1e-10 include-nonctrl --make-bed --out %CD%/data/Output/plink/1-QC/3-HWE/hardy_weinberg_1', ignore.stdout = !verbose)
```

### Removing individuals with a heterozygosity rate deviating more than 3SD from the mean

Deviations can indicate sample contamination or inbreeding. Heterozygosity checks are performed on a set of SNPs that are not highly correlated. To generate this list of non-(highly)correlated SNPs, we exclude high inversion regions.

```{bash indep_SNPs, eval=is.unix, echo=is.unix}
echo "$PWD"
mkdir "$PWD"/data/Output/plink/1-QC/4-Het
plink --bfile "$PWD"/data/Output/plink/1-QC/3-HWE/hardy_weinberg_1 --exclude "$PWD"/data/Input/inversion.txt --range --indep-pairwise 50 5 0.2 --out "$PWD"/data/Output/plink/1-QC/4-Het/indepSNP
```

```{r indep_SNPs2, echo=is.windows, eval=is.windows}
run_shell('mkdir %CD%/data/Output/plink/1-QC/4-Het')
run_shell('plink --bfile %CD%/data/Output/plink/1-QC/3-HWE/hardy_weinberg_1 --exclude %CD%/data/Input/inversion.txt --range --indep-pairwise 50 5 0.2 --out %CD%/data/Output/plink/1-QC/4-Het/indepSNP', ignore.stdout = !verbose)
```

Once we've done that we can compute the heterozygosity rates

```{bash hete_rates, eval=is.unix, echo=is.unix}
plink --bfile "$PWD"/data/Output/plink/1-QC/3-HWE/hardy_weinberg_1 --extract "$PWD"/data/Output/plink/1-QC/4-Het/indepSNP.prune.in --het --out "$PWD"/data/Output/plink/1-QC/4-Het/Het_check
```

```{r hete_rates2, echo=is.windows, eval=is.windows}
run_shell('plink --bfile %CD%/data/Output/plink/1-QC/3-HWE/hardy_weinberg_1 --extract %CD%/data/Output/plink/1-QC/4-Het/indepSNP.prune.in --het --out %CD%/data/Output/plink/1-QC/4-Het/Het_check', ignore.stdout = !verbose)
```

From this file we can generate a list of individuals who deviate more than 3 standard deviations from the heterozigosity rate mean.

```{r generate_het_outliers}
# All credit to Andries Marees at https://github.com/MareesAT/GWA_tutorial
het <- read.table(file.path(output, "plink", "1-QC", "4-Het", "Het_check.het"), head = TRUE)
het$HET_RATE <-  (het$"N.NM." - het$"O.HOM.")/het$"N.NM."
het_fail <-  subset(het, (het$HET_RATE < mean(het$HET_RATE) - 3*sd(het$HET_RATE)) | (het$HET_RATE > mean(het$HET_RATE) + 3*sd(het$HET_RATE)))
het_fail$HET_DST <-  (het_fail$HET_RATE - mean(het$HET_RATE))/sd(het$HET_RATE)
write.table(het_fail[,1:2], file.path(output, "plink", "1-QC", "4-Het", "fail-het-qc.txt"), row.names = FALSE, quote = F)
```

Now, we proceed to remove those heterozygosity rate outliers

```{bash remove_het_outliers, eval=is.unix, echo=is.unix}
plink --bfile "$PWD"/data/Output/plink/1-QC/3-HWE/hardy_weinberg_1 --remove "$PWD"/data/Output/plink/1-QC/4-Het/fail-het-qc.txt --make-bed --out "$PWD"/data/Output/plink/1-QC/4-Het/heterozigosity
```

```{r remove_het_outliers2, eval = is.windows, echo=is.windows}
run_shell('plink --bfile %CD%/data/Output/plink/1-QC/3-HWE/hardy_weinberg_1 --remove %CD%/data/Output/plink/1-QC/4-Het/fail-het-qc.txt --make-bed --out %CD%/data/Output/plink/1-QC/4-Het/heterozigosity', ignore.stdout = !verbose)
```

### Population structure analysis

We can also check for population substrure in our dataset, for instance by performing a PCA. 

*This additional analysis is only actually performed on Unix computers, if in windows I have just provided the commands used and and example of the output obtained*

For that, we downloaded a 1000 genomes file of 629 individuals which were genotyped in blablabla SNPs. This file was downloaded from here: ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/release/20100804/ALL.2of4intersection.20100804.genotypes.vcf.gz. 

After downloading it the file was converted to plink files, and performed QC as described in the Population Stratification tutorial in https://github.com/MareesAT/GWA_tutorial. 

#### Removing unshared SNPs

Just the snps present in both datasets were kept to be able to make the comparison. Due to file size, of the thousand genomes files, we've already extracted the relevant SNPs from the thousand genomes plink files. To do so for the EP dataset we might do:

```{bash, eval = is.unix}
mkdir -p "$PWD"/data/Output/plink/2-POP/
awk '{print$2}' "$PWD"/data/Input/1KG_PCA/1KG.bim > "$PWD"/data/Output/plink/2-POP/1KG_snps.txt
plink --bfile "$PWD"/data/Output/plink/1-QC/4-Het/heterozigosity --extract "$PWD"/data/Output/plink/2-POP/1KG_snps.txt --make-bed --recode --out "$PWD"/data/Output/plink/2-POP/EP
cat "$PWD"/data/Output/plink/2-POP/1KG_snps.txt | wc -l
```

#### Update build

Then, the next step would be to check that both datasets have the same build and if not update the older 1KG file accordingly.

```{bash, eval = is.unix}
awk '{print$2,$4}' "$PWD"/data/Output/plink/2-POP/EP.map > "$PWD"/data/Output/plink/2-POP/buildEP.txt
plink --bfile "$PWD"/data/Output/plink/2-POP/1KG --update-map "$PWD"/data/Output/plink/2-POP/buildEP.txt --make-bed --out "$PWD"/data/Output/plink/2-POP/1KG_PCA
```

#### Prepare merging

In order to merge both files, first we have to ascertain that both files refer to the same reference and alternative alleles.

```{bash, eval = is.unix}
awk '{print$2,$5}' "$PWD"/data/Output/plink/2-POP/1KG_PCA.bim > "$PWD"/data/Output/plink/2-POP/1KG_ref-list.txt
plink --bfile "$PWD"/data/Output/plink/2-POP/EP --reference-allele "$PWD"/data/Output/plink/2-POP/1KG_ref-list.txt --make-bed --out "$PWD"/data/Output/plink/2-POP/EP_adj
awk '{print$2,$5,$6}' "$PWD"/data/Output/plink/2-POP/EP_adj.bim > "$PWD"/data/Output/plink/2-POP/EP_adj_tmp
awk '{print$2,$5,$6}' "$PWD"/data/Output/plink/2-POP/1KG_PCA.bim > "$PWD"/data/Output/plink/2-POP/1KG_PCA_tmp
sort "$PWD"/data/Output/plink/2-POP/1KG_PCA_tmp "$PWD"/data/Output/plink/2-POP/EP_adj_tmp | uniq -u > "$PWD"/data/Output/plink/2-POP/all_differences.txt
cat "$PWD"/data/Output/plink/2-POP/all_differences.txt
```

Those differences might be just caused by looking at different strands so we will flip those bases and see if the error persists.

```{bash, eval = is.unix}
awk '{print$1}' "$PWD"/data/Output/plink/2-POP/all_differences.txt | sort -u > "$PWD"/data/Output/plink/2-POP/flip_list.txt
plink --bfile "$PWD"/data/Output/plink/2-POP/EP_adj --flip "$PWD"/data/Output/plink/2-POP/flip_list.txt --reference-allele "$PWD"/data/Output/plink/2-POP/1KG_ref-list.txt --make-bed --out "$PWD"/data/Output/plink/2-POP/EP_adj_2

## Now check if they are still problematic
awk '{print$2,$5,$6}' "$PWD"/data/Output/plink/2-POP/EP_adj_2.bim > "$PWD"/data/Output/plink/2-POP/EP_adj_2_tmp
sort "$PWD"/data/Output/plink/2-POP/1KG_PCA_tmp "$PWD"/data/Output/plink/2-POP/EP_adj_2_tmp | uniq -u > "$PWD"/data/Output/plink/2-POP/still_differences.txt
cat "$PWD"/data/Output/plink/2-POP/still_differences.txt
```

#### Merge

And we can see that there are no remaining differences anymore so we shouldn't remove any SNP and we can proceed to merge both files.

```{bash, eval = is.unix}
plink --bfile "$PWD"/data/Output/plink/2-POP/EP_adj_2 --bmerge "$PWD"/data/Output/plink/2-POP/1KG_PCA.bed "$PWD"/data/Output/plink/2-POP/1KG_PCA.bim "$PWD"/data/Output/plink/2-POP/1KG_PCA.fam --allow-no-sex --make-bed --out "$PWD"/data/Output/plink/2-POP/EP_1KG_PCA
```


#### Labeling the superpopulations

Before doing the PCA we can label the data according to different human superpopulations (AFR, EUR etc) in order to be able to keep track of them when we plot them graphically. For that we get a file with population information from the 1000 genomes and transform the subpopulation codes in our files in superpopulation ones.

```{bash, eval = is.unix}
wget -P "$PWD"/data/Output/plink/2-POP/ ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20100804/20100804.ALL.panel

## rename populations
awk '{print$1,$1,$2}' "$PWD"/data/Output/plink/2-POP/20100804.ALL.panel > "$PWD"/data/Output/plink/2-POP/pop_1kG.txt
sed 's/JPT/ASN/g' "$PWD"/data/Output/plink/2-POP/pop_1kG.txt > "$PWD"/data/Output/plink/2-POP/pop_1kG2.txt
sed 's/ASW/AFR/g' "$PWD"/data/Output/plink/2-POP/pop_1kG2.txt > "$PWD"/data/Output/plink/2-POP/pop_1kG3.txt
sed 's/CEU/EUR/g' "$PWD"/data/Output/plink/2-POP/pop_1kG3.txt > "$PWD"/data/Output/plink/2-POP/pop_1kG4.txt
sed 's/CHB/ASN/g' "$PWD"/data/Output/plink/2-POP/pop_1kG4.txt > "$PWD"/data/Output/plink/2-POP/pop_1kG5.txt
sed 's/CHD/ASN/g' "$PWD"/data/Output/plink/2-POP/pop_1kG5.txt>"$PWD"/data/Output/plink/2-POP/pop_1kG6.txt
sed 's/YRI/AFR/g' "$PWD"/data/Output/plink/2-POP/pop_1kG6.txt>"$PWD"/data/Output/plink/2-POP/pop_1kG7.txt
sed 's/LWK/AFR/g' "$PWD"/data/Output/plink/2-POP/pop_1kG7.txt>"$PWD"/data/Output/plink/2-POP/pop_1kG8.txt
sed 's/TSI/EUR/g' "$PWD"/data/Output/plink/2-POP/pop_1kG8.txt>"$PWD"/data/Output/plink/2-POP/pop_1kG9.txt
sed 's/MXL/AMR/g' "$PWD"/data/Output/plink/2-POP/pop_1kG9.txt>"$PWD"/data/Output/plink/2-POP/pop_1kG10.txt
sed 's/GBR/EUR/g' "$PWD"/data/Output/plink/2-POP/pop_1kG10.txt>"$PWD"/data/Output/plink/2-POP/pop_1kG11.txt
sed 's/FIN/EUR/g' "$PWD"/data/Output/plink/2-POP/pop_1kG11.txt>"$PWD"/data/Output/plink/2-POP/pop_1kG12.txt
sed 's/CHS/ASN/g' "$PWD"/data/Output/plink/2-POP/pop_1kG12.txt>"$PWD"/data/Output/plink/2-POP/pop_1kG13.txt
sed 's/PUR/AMR/g' "$PWD"/data/Output/plink/2-POP/pop_1kG13.txt>"$PWD"/data/Output/plink/2-POP/pop_1kG14.txt

# Create our own population file
awk '{print$1,$2,"EP"}' "$PWD"/data/Output/plink/2-POP/EP_adj_2.fam > "$PWD"/data/Output/plink/2-POP/pop_EP.txt

# Concatenate population files.
cat "$PWD"/data/Output/plink/2-POP/pop_1kG14.txt "$PWD"/data/Output/plink/2-POP/pop_EP.txt | sed -e '1i\FID IID SUPERPOP' > "$PWD"/data/Output/plink/2-POP/popfile.txt

# remove temporary files from above
rm -f "$PWD"/data/Output/plink/2-POP/pop_*.txt
```

#### PCA

With the data merged we can perform PCA on plink.

```{bash, eval = is.unix}
plink --bfile "$PWD"/data/Output/plink/2-POP/EP_1KG_PCA --pca --out "$PWD"/data/Output/plink/2-POP/PCA_results
```

We will plot the results using a small Python script (all credit to Xavier Farré)

```{python, eval = is.unix}
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import logging

df = pd.read_csv('data/Output/plink/2-POP/PCA_results.eigenvec', delim_whitespace=True, header=None) 

cols = ['FID', 'IID']
for i in range(1, 21):
    cols.append(f'PC{i}')

df.columns = cols

colorcode = pd.read_csv('data/Output/plink/2-POP/popfile.txt', sep=' ')


df = pd.merge(df, colorcode, on=['FID', 'IID'])


ax = sns.lmplot('PC1', # Horizontal axis
           'PC2', # Vertical axis
           hue = 'SUPERPOP',  # color variable
           data=df, # Data source
           fit_reg=False, # Don't fix a regression line
           height = 10,
           aspect =2 ) # height and dimension

plt.title('PCA: Projection on 1000Genomes')
# Set x-axis label
plt.xlabel(f'PC1')
# Set y-axis label
plt.ylabel('PC2')

plt.savefig('data/PCA.png', dpi=240, bbox_inches='tight')


def label_point(x, y, val, ax):
    a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)
    for i, point in a.iterrows():
        ax.text(point['x']+.0001, point['y'], str(point['val']))

label_point(df[df['SUPERPOP'] == 'EP']['PC1'], df[df['SUPERPOP'] == 'EP']['PC2'], df[df['SUPERPOP'] == 'EP']['IID'], plt.gca())

plt.savefig('data/PCA_labels.png', dpi=240, bbox_inches='tight')

```

```{r PCA_plot, echo=FALSE, fig.cap="PCA performed on the EP dataset", out.width = '100%'}
knitr::include_graphics(file.path(getwd(), "data", "PCA.png"))
```

## Defining inheritance models

The next step is to obtaining all possible snp inheritance model combinations and adding the new columns to the dataset.

```{r inheritance_models}
recessive <- paste0(vector_of_snps, "r")
additive <- paste0(vector_of_snps, "a")
dominant <- paste0(vector_of_snps, "d")
inheritance <- c(recessive, additive, dominant)
All[inheritance] <- NA
```


Then we define the variables (columns) order in the dataset.

```{r col_order}
col_order <- c("ID", "Diag", "Sex", "Age_to_use", "Age82", "Region",  "E4status", "Centre", centres, vector_of_snps, inheritance)
 All <- All[,col_order]
```

Finally we code the inheritance with numbers depending on the genotype values for each snp. (0 reference, 1 risk allele effect, 2 two risk alleles effect {just for the additive model})

```{r inheritances}
All <- generate_inheritances(snps = list_of_objects, data = All)
```

</br>

## Diagnose possible problems and perform data conversions

</br>

After a quick look at the data we decide to codify an unexplained level in the E4status variable (coming from Rotterdam dataset) into missing data.

```{r missing_E4status}
All$E4status[All$E4status == 2] <- NA
```

Then we convert all categorical variables into the factor class.

```{r factors}
factors <- -c(1,4)
All[,factors] <- lapply(All[,factors], factor)
```

The next step to be performed is to do some further data exploration by creating contingency tables of age, sex and E4status by diagnosis and centre.

```{r contingency tables}
variables <- c("Age82", "Sex", "E4status", "Diag", "Centre", "Region")
list_of_tables <- vector(mode = "list", length = 2)
i = 2 # Two pairs of variables cross tabulation
while (i < 4) { 
  # Generate all combinations of variables possible, taking i at a time.
  combinations <- combn(variables, i, simplify = FALSE)
  list_of_tables[[i - 1]] <- character(length = length(combinations))
  # For combination in combinations
  for (n in seq_along(combinations)) {
    combination <- combinations[[n]]
    # Create the formula to input to the xtabs function
    formula <- paste0("All$", combination)
    formula <- paste(formula, collapse = " + ")
    formula <- paste0("~", formula)
    # Create the name of the table as variable1_variable2
    name <- paste(combination, collapse = "_")
    assign(name, xtabs(formula))
    list_of_tables[[i - 1]][n] <- name
  }
  # Once all possible 2 variables combinations have been performed, do it
  # for three variables
  i = i + 1
}
names(list_of_tables) <- c("Two variables", "Three variables")
```

We can take a quick look at an overview of the contingency tables created by printing the list_of_tables object.

```{r list_of_tables}
list_of_tables
```

Another step we can perform for the sake of error checking is creating summary tables of mean, min, max age by diagnosis and centre.

First, we give variables shorter names for ease of use.

```{r shorter_names}
ages <- All$Age_to_use
diagnostic <- All$Diag
centres <- All$Centre
regions <- All$Region
```

Then we create the summary tables.

```{r summary_tables}
### Only Age by diagnosis
variable <- ages
group <- diagnostic
AgeAll <- createSummary(variable, group)

### Age of alzheimer patients by Centre
variable <- ages[diagnostic == "AD"]
group <- centres[diagnostic == "AD"]
AgeAD <- createSummary(variable, group)

### Age of control patients by Centre
variable <- ages[diagnostic == "Control"]
group <- centres[diagnostic == "Control"]
AgeControl <- createSummary(variable, group)

### Age of alzheimer patients by Region (N.Eur or Spain)
variable <- ages[diagnostic == "AD"]
group <- regions[diagnostic == "AD"]
AgeADRegion <- createSummary(variable, group)

### Age of control patients by Region (N.Eur or Spain)
variable <- ages[diagnostic == "Control"]
group <- regions[diagnostic == "Control"]
AgeControlRegion <- createSummary(variable, group)

summary_tables <- c("AgeAll", "AgeAD", "AgeControl", "AgeADRegion", "AgeControlRegion")

for (summary in summary_tables) {
  print(summary)
  print(get(summary))
  cat("\n")
}
```

</br>

## Creating data subsets

</br>

We create a data subset for each of the regions.

```{r region_subsets}
for (region in levels(All$Region)) {
  subset <- subset(All, Region == region)
  assign(region, subset)
}
```

And also for each individual center.

```{r centres_subsets}
for (centre in levels(All$Centre)) {
  subset <- subset(All, Centre == centre)
  centre <- stringr::str_to_title(centre)
  assign(centre, subset)
}
```

Now, that we've done that it would be interesting to also subset according to each one of the covariates.

```{r covariates_subsets}
 variables <- c("Age82", "Sex", "E4status")
for (variable in variables) {
  for (level in levels(All[[variable]])) {
    name <- paste(variable, level, sep = "_")
    subset <- All[All[[variable]] == level,]
    assign(name, subset)
  }
}
```

And also subset the covariates but by region.

```{r region_covariate_subset}
for (region in levels(All$Region)) {
  Region <- get(region)
  for (variable in variables) {
    for (level in levels(Region[[variable]])) {
      name <- paste(region, variable, level, sep = "_")
      subset <- Region[Region[[variable]] == level,]
      assign(name, subset)
    }
  }
}
```

And of course, subsetting those same covariates but by center.

```{r centre_covariate_subset}
centres <- levels(All$Centre)
centres <- stringr::str_to_title(centres)

for (centre in centres) {
  Centre <- get(centre)
  for (variable in variables) {
    for (level in levels(All[[variable]])) {
      name <- paste(centre, variable, level, sep = "_")
      subset <- Centre[Centre[[variable]] == level,]
      assign(name, subset)
    }
  }
}
```

</br>

# Analysis

</br>

## Main effects

</br>

First, we select the datasets into which we want to perform the analysis and then all covariates we want to control for.

```{r datasets&covariates}
DATASETS <- c("All", "N.Eur", "Spain")
variables <- c("Sex", "E4status", "Age82", "Centre")
```

Then we reorder the datasets alphabetically and set Santander as the reference level in the Spanish instead of Madrid as the former has a higher N and also making sure the analysis are performed with respect to the Control reference level not the Diagnosed ones.

```{r reorder_alphabetically}
for (i in seq_along(DATASETS)) {
  assign(DATASETS[i], get(DATASETS[i])[,order(names(get(DATASETS[i])))])
}
Spain$Centre <- relevel(Spain$Centre, ref = 6)
All$Diag <- relevel(All$Diag, ref = "Control")
```

Finally we perform the main effects analysis for the selected SNPs.

```{r main_effects, results="hide"}
master_list <- perform_analysis(.mode = "main_effects", .data = DATASETS, snps = list_of_objects, covariates = variables)
```

This function outputs a list with the results of the glm contained in it (best model of inheritance and glm results). An example of the structure of the list with only the main effects performed can be seen here, consisting in the first snp of the All (N.Eur + Spain) dataset:

```{r master_list, echo=FALSE}
rlist::list.clean(master_list$All[1], recursive = T, fun = function(x) is.null(x) | length(x) == 0)
```

We can appreciate that the main effects analysis were also performed for the Spanish and Northern European subsets.

```{r subsets_master_list}
names(master_list)
```

Then, selecting a threshold of 0.05, before applying multiple testing correction, we can obtain the following results for the snps under the threshold and to which genes it belongs.

```{r print_sig}
print_significant_results(master_list)
```

```{r print_corrected_sig}
print_significant_results(master_list, corrected = T)
```

Here, we can see that both additive levels for snp rs429358 remain significant, so what we really have are 4 snp models that are found significantly associated with AD, even after multiple testing correction.

Those are the APOE gene, for which has the E4 variant was found to be the largest known genetic risk factor for late-onset sporadic AD. The rationale is that this isoform APOE-ε4 is not as effective as the others at promoting these reactions, resulting in increased vulnerability to AD in individuals with that gene variation

The BDNF snp (rs11030102) has been [linked](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3288597/pdf/nihms336587.pdf) to decreased BDNF serum levels.  In the adult brain, BDNF maintains high expression levels and regulates both excitatory and inhibitory synaptic transmission and activity-dependent plasticity ([Miranda et al., 2019](https://www.frontiersin.org/articles/10.3389/fncel.2019.00363/full))

The bridging integrator 1 (BIN1) gene is the second most important susceptibility gene for late-onset Alzheimer’s disease (LOAD) after apolipoprotein E (APOE) gene. ([Hu et al., 2021](https://www.nature.com/articles/s41398-021-01218-9.pdf)). BIN1 encodes a Myc-interacting protein but its role in AD is still unclear.

To see a count of the number of genes studied, according to the number of snps we can use the following code:

```{r genes_studied_count}
genes_studied <- sapply(master_list$All, function(snp) {snp$Gene})
genes_studied <- unname(genes_studied)
genes_studied <- table(genes_studied)
sort(genes_studied, decreasing = T)
```

Here we can see that from the 75 SNPs studied, all 3 APOE snps were found significant as well as the only SNP studied for BIN1. For BDNF, though just one out of five SNPs was deemed significant according to main effects, snp rs11030102.

To measure the strength of the association found we can extract the OR from the main effects results.

```{r significant_snps_OR}
significant_snps <- c("rs11030102d1","rs429358a1", "rs429358a2", "rs7412d1","rs744373a2")
OR_results <- sapply(significant_snps, function(snp_model_lvl){
  snp_model <- sub("[0-2]$", "", snp_model_lvl);
  snp <- sub("[a-z]$", "", snp_model);
  glm_results <- master_list$All[[snp]]$Main_effects[[snp_model]]
  index <- grep(rownames(glm_results), pattern = snp_model_lvl)
  glm_results[index,][3]
})
snps <- sub(names(OR_results), pattern = "[a-z][0-2].OR$", replacement = "")
genes <- sapply(snps, function(snp) {list_of_objects[[snp]]$gene})
OR_results2 <- OR_results
names(OR_results2) <- genes
sort(OR_results, decreasing = T)
sort(OR_results2, decreasing = T)
```

Thus, we can see that the additive model for snp rs429358, which corresponds to the APOE gene increases the odds of having 1.7 times with respect no not having the variant allele, while the full dosage increases it 8 times. 

On the other hand, the others, hold more moderate effects, with the dominant model of snp rs7412d, which also corresponds to APOE, increases the odds of having the disease 0.682. Or what is the same, decreases it by 1/0.682 = `r 1/0.682`

We can generate a dataframe to plot these results:

```{r generate_plot}
plot_df <- data.frame(matrix(NA, nrow = length(significant_snps), ncol = 6))
colnames(plot_df) <- c("snp", "gene", "OR", "lower", "upper", "pvalue")
for (i in seq_along(significant_snps)) {
  snp_model_lvl <- significant_snps[i]
  snp_model <- sub(snp_model_lvl, pattern = "[0-2]$", replacement = "")
  snp <- sub(snp_model, pattern = "[a-z]$", replacement = "")
  SNP <- master_list$All[[snp]]
  gene <- SNP$Gene
  main_effects <- SNP$Main_effects[[snp_model]]
  snp_term <- grep(pattern = snp_model_lvl, x = rownames(main_effects))
  OR <- main_effects[snp_term, 3]
  lower <- main_effects[snp_term, 4]
  upper <- main_effects[snp_term, 5]
  pvalue <- main_effects[snp_term, 6]
  plot_df[i,1] <- snp_model_lvl
  plot_df[i,2] <- gene
  plot_df[i,3] <- OR
  plot_df[i,4] <- lower
  plot_df[i,5] <- upper
  plot_df[i,6] <- pvalue
}
```

And plot them:

```{r plot_ME, echo=FALSE}
p <- ggplot(data = plot_df, aes(x = OR, y = snp, xmin = lower, xmax = upper))
p <- p + geom_pointrange(aes(col = gene))
p <- p + geom_vline(aes(fill = gene), xintercept = 1, linetype = "dotted")
p <- p + geom_errorbar(aes(xmin = lower, xmax = upper, col = gene),width = 0.5, cex = 1)
p <- p + scale_x_log10(breaks = c(0.5, 1, 2, 4, 8, 16))
p
```


</br>

## Covariate interaction

</br>

The next step in the analysis would be to examine all possible interactions of the 75 studied SNPs with the covariates Age82, Sex & E4status.

In practice this means repeating the models from the main effects but by adding one interaction term at a time.

In order to reduce the multiple testing performed and due to the previous findings of what the best models were, only the best snp models chosen for each main effect were employed in this subsequent step.

```{r covariates_interactions, results='hide'}
master_list <- perform_analysis(.mode = "interaction", .data = DATASETS, snps = list_of_objects, covariates = variables)
```

And here we can see an example of the kind of results obtained by doing this analysis.

```{r covar_interactions_example}
master_list$All[[1]]$Interactions$Other_covariates
```

First, for the variable "Name" we have the interaction studied, then as "Summary" the full glm model results obtained, and the rest of variables, extract the most interesting results for the interaction, the "SF" and whether it is found "Significant" or not.

The SF is a statistic used to measure interactions in complex diseases. [It is defined](https://bmcresnotes.biomedcentral.com/track/pdf/10.1186/1756-0500-2-105.pdf) as the OR of the interaction divided by the product of the OR of each of the interaction on its own.

Thus, it makes a good and easy to understand metric for what we are measuring as if there was no interaction effect we would expect the SF to be close to 1, as the impact of the interaction should be about the same as the effect of each of the members on its on. However, if the interaction is found greater or lower than 1, that means we are having a synergistic effect in which the odds on having the disease is increased or reduced respectively.

Having said that, we can obtain the significant results found for this snp-covariate interaction measured:




